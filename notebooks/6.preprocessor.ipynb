{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "from news_summarizer.domain.documents import Article\n",
    "from news_summarizer.domain.clean_documents import CleanedArticle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "class TextTransformation:\n",
    "    def apply(self, text: str) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class StripWhitespace(TextTransformation):\n",
    "    def apply(self, text: str) -> str:\n",
    "        return text.strip()\n",
    "\n",
    "class RemoveEmojis(TextTransformation):\n",
    "    def apply(self, text: str) -> str:\n",
    "        emoji_pattern = re.compile(\n",
    "            \"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', text)\n",
    "\n",
    "class RemoveNonAsciiExceptAccents(TextTransformation):\n",
    "    def apply(self, text: str) -> str:\n",
    "        return ''.join(\n",
    "            c for c in text\n",
    "            if ord(c) < 128 or unicodedata.category(c).startswith('L')\n",
    "        )\n",
    "\n",
    "class ReplaceMultipleSpaces(TextTransformation): \n",
    "    def apply(self, text: str) -> str: \n",
    "        return re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "class TextPipeline:\n",
    "    def __init__(self):\n",
    "        self.transformations = []\n",
    "\n",
    "    def add_transformation(self, transformation: TextTransformation):\n",
    "        self.transformations.append(transformation)\n",
    "\n",
    "    def execute(self, text: str) -> str:\n",
    "        if text is None:\n",
    "            return ''\n",
    "        for transformation in self.transformations:\n",
    "            text = transformation.apply(text)\n",
    "        return text\n",
    "\n",
    "# Example usage\n",
    "pipeline = TextPipeline()\n",
    "pipeline.add_transformation(StripWhitespace())\n",
    "pipeline.add_transformation(RemoveEmojis())\n",
    "pipeline.add_transformation(RemoveNonAsciiExceptAccents())\n",
    "pipeline.add_transformation(ReplaceMultipleSpaces())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_list = list(Article.bulk_find(**{}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_documents_list = []\n",
    "\n",
    "for i, article in enumerate(document_list):\n",
    "    cleaned_article = CleanedArticle(\n",
    "        id=article.id,\n",
    "        title=pipeline.execute(article.title),\n",
    "        author=article.author,\n",
    "        content=pipeline.execute(article.content),\n",
    "        subtitle=pipeline.execute(article.subtitle),\n",
    "        publication_date=article.publication_date,\n",
    "        url=article.url\n",
    "    )\n",
    "\n",
    "    cleaned_documents_list.append(cleaned_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator\n",
    "\n",
    "def batch(list_: list, size: int) -> Generator[list, None, None]:\n",
    "    yield from (list_[i : i + size] for i in range(0, len(list_), size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from uuid import UUID\n",
    "import hashlib\n",
    "from typing import List, Union\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
    "from news_summarizer.domain.chunks import ArticleChunk\n",
    "from news_summarizer.domain.clean_documents import CleanedArticle\n",
    "from news_summarizer.domain.embeddeg_chunks import EmbeddedArticleChunk\n",
    "\n",
    "class ChunkingService:\n",
    "    def __init__(\n",
    "        self,\n",
    "        separators: List[str],\n",
    "        character_chunk_size: int = 250,\n",
    "        character_chunk_overlap: int = 0,\n",
    "        token_chunk_size: int = 128,\n",
    "        token_chunk_overlap: int = 25,\n",
    "        token_model_name: str = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the ChunkingService with configuration parameters.\n",
    "        \n",
    "        Args:\n",
    "            separators (List[str]): List of separators for character-based splitting.\n",
    "            character_chunk_size (int): Size of chunks for character splitting.\n",
    "            character_chunk_overlap (int): Overlap between character chunks.\n",
    "            token_chunk_size (int): Maximum token count per chunk for token splitting.\n",
    "            token_chunk_overlap (int): Overlap between token chunks.\n",
    "            token_model_name (str): Name of the model for token-based splitting.\n",
    "        \"\"\"\n",
    "        self.character_splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=separators,\n",
    "            chunk_size=character_chunk_size,\n",
    "            chunk_overlap=character_chunk_overlap,\n",
    "        )\n",
    "        self.token_splitter = SentenceTransformersTokenTextSplitter(\n",
    "            chunk_overlap=token_chunk_overlap,\n",
    "            tokens_per_chunk=token_chunk_size,\n",
    "            model_name=token_model_name,\n",
    "        )\n",
    "        self.character_chunk_size = character_chunk_size\n",
    "        self.token_chunk_overlap = token_chunk_overlap\n",
    "\n",
    "    def split_text_into_chunks(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Splits a given text into chunks based on characters and tokens.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The text to split into chunks.\n",
    "        \n",
    "        Returns:\n",
    "            List[str]: A list of text chunks.\n",
    "        \"\"\"\n",
    "        # Step 1: Split text into character-based chunks\n",
    "        character_chunks = self.character_splitter.split_text(text)\n",
    "\n",
    "        # Step 2: Further split character chunks into token-based chunks\n",
    "        token_chunks = []\n",
    "        for chunk in character_chunks:\n",
    "            token_chunks.extend(self.token_splitter.split_text(chunk))\n",
    "        \n",
    "        return token_chunks\n",
    "\n",
    "    def chunk(self, data_model: CleanedArticle) -> List[ArticleChunk]:\n",
    "        \"\"\"\n",
    "        Converts a CleanedArticle into a list of ArticleChunks.\n",
    "        \n",
    "        Args:\n",
    "            data_model (CleanedArticle): The article to chunk.\n",
    "        \n",
    "        Returns:\n",
    "            List[ArticleChunk]: The list of resulting ArticleChunks.\n",
    "        \"\"\"\n",
    "        # Generate text chunks\n",
    "        text_chunks = self.split_text_into_chunks(data_model.content)\n",
    "\n",
    "        # Convert text chunks into ArticleChunk models\n",
    "        article_chunks = []\n",
    "        for chunk in text_chunks:\n",
    "            chunk_id = hashlib.md5(chunk.encode()).hexdigest()\n",
    "            article_chunk = ArticleChunk(\n",
    "                id=UUID(chunk_id, version=4),\n",
    "                title=data_model.title,\n",
    "                subtitle=data_model.subtitle,\n",
    "                content=chunk,\n",
    "                author=data_model.author,\n",
    "                publication_date=data_model.publication_date,\n",
    "                url=data_model.url,\n",
    "                document_id=data_model.id,\n",
    "                metadata={\n",
    "                    \"chunk_size\": self.character_chunk_size,\n",
    "                    \"chunk_overlap\": self.token_chunk_overlap,\n",
    "                },\n",
    "            )\n",
    "            article_chunks.append(article_chunk)\n",
    "        \n",
    "        return article_chunks\n",
    "\n",
    "class EmbedderService:\n",
    "    def __init__(self, embedder):\n",
    "        \"\"\"\n",
    "        Initializes the EmbedderService with the embedding model.\n",
    "        \n",
    "        Args:\n",
    "            embedder: An embedding model instance with attributes like `model_id` and a callable for generating embeddings.\n",
    "        \"\"\"\n",
    "        self.embedder = embedder\n",
    "\n",
    "    def create_embedded_chunk(self, data_model: ArticleChunk, embedding: List[float]) -> EmbeddedArticleChunk:\n",
    "        \"\"\"\n",
    "        Creates an EmbeddedArticleChunk from a given ArticleChunk and its embedding.\n",
    "        \"\"\"\n",
    "        return EmbeddedArticleChunk(\n",
    "            id=data_model.id,\n",
    "            title=data_model.title,\n",
    "            subtitle=data_model.subtitle,\n",
    "            author=data_model.author,\n",
    "            content=data_model.content,\n",
    "            url=data_model.url,\n",
    "            document_id=data_model.document_id,\n",
    "            embedding=embedding,\n",
    "            metadata={\n",
    "                \"embedding_model_id\": self.embedder.model_id,\n",
    "                \"embedding_size\": self.embedder.embedding_size,\n",
    "                \"max_input_length\": self.embedder.max_input_length,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def embed_batch(self, data_models: List[ArticleChunk]) -> List[EmbeddedArticleChunk]:\n",
    "        \"\"\"\n",
    "        Embeds a batch of ArticleChunks into EmbeddedArticleChunks.\n",
    "        \"\"\"\n",
    "        embedding_inputs = [chunk.content for chunk in data_models]\n",
    "        embeddings = self.embedder(embedding_inputs, to_list=True)\n",
    "        return [\n",
    "            self.create_embedded_chunk(chunk, embedding)\n",
    "            for chunk, embedding in zip(data_models, embeddings, strict=False)\n",
    "        ]\n",
    "\n",
    "    def embed(self, data_model: Union[ArticleChunk, List[ArticleChunk]]) -> Union[EmbeddedArticleChunk, List[EmbeddedArticleChunk]]:\n",
    "        \"\"\"\n",
    "        Embeds one or more ArticleChunks.\n",
    "        \"\"\"\n",
    "        is_single_instance = not isinstance(data_model, list)\n",
    "        data_models = [data_model] if is_single_instance else data_model\n",
    "        embedded_chunks = self.embed_batch(data_models)\n",
    "        return embedded_chunks[0] if is_single_instance else embedded_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from news_summarizer.embeddings import EmbeddingModel\n",
    "\n",
    "separators = [\n",
    "    \"\\n\\n\",  # Paragraph breaks\n",
    "    \"\\n\",    # Line breaks\n",
    "    \" \",     # Spaces\n",
    "    \".\",     # Periods\n",
    "    \",\",     # Commas\n",
    "    \"!\",     # Exclamation marks\n",
    "    \"?\",     # Question marks\n",
    "    \";\",     # Semicolons\n",
    "    \":\",     # Colons\n",
    "    \"\\u2026\",  # Ellipsis (â€¦)\n",
    "    \"\\u00A0\",  # Non-breaking space\n",
    "]\n",
    "\n",
    "embedder = EmbeddingModel(model_id='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', device='cuda', cache_dir=None)\n",
    "chunking_service = ChunkingService(separators=separators)\n",
    "embedder_service = EmbedderService(embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_documents = cleaned_documents_list\n",
    "\n",
    "def chunk_and_embed(cleaned_documents):\n",
    "    embedded_chunks = []\n",
    "    for document in cleaned_documents:\n",
    "        chunks = chunking_service.chunk(document)\n",
    "        \n",
    "        for batched_chunks in batch(chunks, 10):\n",
    "            batched_embedded_chunks = embedder_service.embed(batched_chunks)\n",
    "            embedded_chunks.extend(batched_embedded_chunks)\n",
    "\n",
    "    return embedded_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_chunks = chunk_and_embed(cleaned_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from news_summarizer.database.qdrant import connection\n",
    "#connection.delete_collection(EmbeddedArticleChunk.Config.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EmbeddedArticleChunk.bulk_insert(embedded_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_embeddings():\n",
    "    offset = None\n",
    "    my_embeddings = []\n",
    "\n",
    "    while True:\n",
    "        embeddings, offset = EmbeddedArticleChunk.bulk_find(**{}, offset=offset)\n",
    "        my_embeddings.extend(embeddings)\n",
    "\n",
    "        if offset is None:\n",
    "            break\n",
    "\n",
    "    return my_embeddings\n",
    "\n",
    "# Example usage\n",
    "all_embeddings = fetch_all_embeddings()\n",
    "len(all_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news_summarizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
